### The CNN
Cnn. 
some text
Всем привет, меня зовут Хворостяный Вячеслав. В этой статье я продолжу тему имплементации базовых алгоритмов "машинного обучения" с помощью Python. В предыдущей статье был рассмотрен алгоритм линейной регрессии, а также такой метод оптимизации как градиентный спуск. В этой статье разберем логистическую регрессию, регуляризацию и применения алгоритмов на практике.

Существует два основных типа алгоритмов "машинного обучения" - "обучение с учителем" ("supervised learning") и "обучение без учителя" ("unsupervised learning"). В свою очередь, "обучение с учителем" делиться на регрессионные алгоритмы и алгоритмы классификации. Логистическая регрессия принадлежит к типу "supervised learning", и, несмотря на название, является алгоритмом классификации, часто используемым как функция активации в нейронных сетях, или как самостоятельный метод классификации объектов. "Обучение с учителем" значит, что у нас уже есть некоторое количество данных где известны как значение аргументов, так и значения самой функции, и наша задача состоит в том, чтобы "обучится" на этих, уже размеченных данных.

В основе алгоритма логистической регрессии лежит экспоненциальная кривая или сигмоида, которая разделяет объекты на два класса.

$$z^{(i)} = w^T x^{(i)} + b $$

$$\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$

$$sigmoid(z^{(i)}) = \frac{1}{1-e^{-z^{(i)}}}$$

Где
$$z^{(i)}$$ - функция прямой (линейная регрессия)
$$\hat{y}^{(i)}$$ - гипотеза
$$sigmoid(z^{(i)})$$ - сигмоида

Задача алгоритма - вычислить вероятность того, что элемент принадлежит к одному из классов. Значения функции находятся в границах от [0,1], самый простой способ - округлить все значения больше 0.5 до единицы, порог можно задать и выше, в том случа
