### Имплементация компьютерной нейронной сети с помощью Python  
{% include math.html %}
Всем привет. [В предыдущей статье](https://vkhvorostianyi.github.io/2019/04/14/logistic-regression-and-regularization.html) уже было упомянуто о том, что функция сигмоиды может использоваться как фукция активации компьютерной "нейронной сети", сегодня рассмотрим алгоритм "нейронки" более подробно. Компьютерная "нейронная сеть" (Computer Neural Network) является алгоритмом машинного обучения, который за своим принципом напоминает работу нейронной сети человека. В двух словах условный "нейрон" - это функция (активации), которая принимает и отдает сигналы.

Изображение "нейрона" (математическая модель парсептрона)
![img](/assets/neuron.png)

Нейроны-функции обьеденяются в слои, слоев может быть много и не очень, их расделяют на входной, скрытый(скрытые) и выходной, при этом нейронная сеть с одним скрытым (внутренним) слоем будет називатся однослойной.  
Виглядит это примерно так:  

![img](/assets/neuronnaya-set.gif)

Нужно добавить, что рассматриваемая нейронная сеть относится к типу "обучение с учителем" (supervised learning) и решает задачу классификации. 


#### Функция активации

Рассмотрим некоторые популярные функции активации.

**Сигмоида**

Как было сказано выше, функция сигмоиды часто используется как функция активации в нейронных сетях, наряду с гиперболическим тангенсои и ReLU(о которых поговорим ниже).

Ниже дана формула сигмоиды (в качестве примера степенной функции взята линейная регрессия):

$$z^{(i)} = w^T x^{(i)} + b $$
 
$$\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$

$$sigmoid(z^{(i)}) = \frac{1}{1-e^{-z^{(i)}}}$$


Где   
$$z^{(i)}$$ - степенная функция (линейная регрессия)  
$$\hat{y}^{(i)}$$ - гипотеза   
$$sigmoid(z^{(i)})$$ - сигмоида 

Графичек:  
![img](/assets/sigmoid.png)

Преимущества сигмоиды:
- Гладкость: Имеет производную в любой точке, что является важным фактором при использовании метода градиетного спуска.  

- Чуствительна к малейшим изменениям $$x$$ на участке от [-2;2], что позволяет четко провести границу между классами.  

- Значения $$y$$ всегда находятся в диапазоне [0,1]

Недостатки:
- Стоимость вычислений, критично на устроиствах с ограниченымы ресурсами(IoT).  

- Если в промежутке [-2,2] кривая сигмоиды изменяет значения "быстро" то по краях фукция остается практически неизменной, это говорит о том, что при высоких порогах, будет крайне сложно получить значения, которые близки к еденице, нейронная сеть будет обучаться долго и малоэфективно. Эта проблема также известна, как проблема изчезающего градиента, так как ближе к границам сигмоидальной функции, градиент стремится к нулю, что в свою очередь рано или поздно приведет к проблемам с плавающей точкой.  

**Гиперболический тангенс**

Гиперболический тангенс - еще одна распространенная функция активации.

![img](/assets/tanh.png)

Формула тангенса:
$$tanh(x) = 2sigmoid(2x) - 1$$  
Где   
  
$$sigmoid(z^{(i)}) = \frac{1}{1-e^{-z^{(i)}}}$$
  
По своей сути гиперболический тангенс является модифицированной сигмоидой.
Для сравнения покажем на одном графике:

![img](/assets/tanh_vs_sigmoid.png)

Как можно увидеть на графике, кривая тангенса круче, соответсвенно, более чувствительна к изменениям $$x$$. Также стоит заметить, что значения функции находятся в границах [-1,1], то есть, $$tanh$$ имеет большую амплитуду,что является преимуществом, так как значения $$y$$ для каждого класса лежит в более широком диапазоне.

Функция тангенса подвержена тем же порокам, что и сигмоида, так как имеет с ней общую сущность.
На практике активно применяют и сигмоиду и гиперболический тангенс, а что выбрать зависит от конкретной ситуации.

**ReLU**

ReLU(rectified linear unit) - наиболее часто применяемaя функция активации, в частности в области "компьютерного зрения". 

$$f(x) = max(0,x)$$

ReLu возвращает значение х, если х положительно, и 0 в противном случае. 

![img](/assets/relu.png)
 

Основное преимущество этой функции - разреженность активации. В случае сигмоиды или $$tanh$$ все нейроны частично активированы, в то время как ReLU позволяет не активировать некоторые вообще, что ведет к снижению вичислительных затрат, а это как нельзя актуально при обучении глубоких нейронных сетей.

Правда есть и недостатки: 
>Из-за того, что часть ReLu представляет из себя горизонтальную линию (для отрицательных значений X), градиент на этой части равен 0. Из-за равенства нулю градиента, веса не будут корректироваться во время спуска. Это означает, что пребывающие в таком состоянии нейроны не будут реагировать на изменения в ошибке/входных данных (просто потому, что градиент равен нулю, ничего не будет меняться). Такое явление называется проблемой умирающего ReLu (Dying ReLu problem). Из-за этой проблемы некоторые нейроны просто выключатся и не будут отвечать, делая значительную часть нейросети пассивной. Однако существуют вариации ReLu, которые помогают эту проблему избежать. Например, имеет смысл заменить горизонтальную часть функции на линейную. Если выражение для линейной функции задается выражением y = 0.01x для области x < 0, линия слегка отклоняется от горизонтального положения. Существует и другие способы избежать нулевого градиента. Основная идея >здесь — сделать градиент неравным нулю и постепенно восстанавливать его во время тренировки. 

Вот как это выглядит:

![img](/assets/leaky_relu.png)

Графики различных ф-ций активации:  

![img](/assets/actviation_func.png)




#### Прямое распространение (Forward Propagation)  

Прямое распостранение - это собстевенно, пропуск данных через мясорубку скрытых слоев нейронов.
Например если у нас три скрытых слоя то, прямое распространие можно описать так:

$$f(x) = f_3(f_2(f_1(x)))$$   

Где:

$$f_1(x)$$: функция выученая на первом слое  
$$f_2(x)$$: функция выученая на втором слое  
$$f_3(x)$$: функция выученая на третьем слое  

#### Обратное распостранение

Обратное распостранение представляет собой подсчет ошибки на каждом из слоев и коректирование весов
с целью минимизации этой ошибки.

Для этого мы подсчитываем градиент на каждом слое и умножаем эго на  скорость обучения(learning rate), штрафуем этим значением вектор параметров для каждого слоя, в итоге добиваясь минимизации ошибки.

![img](/assets/backprop.jpeg)



Ссылки:
https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/  
https://neurohive.io/ru/osnovy-data-science/activation-functions/  
https://neurohive.io/ru/osnovy-data-science/osnovy-nejronnyh-setej-algoritmy-obuchenie-funkcii-aktivacii-i-poteri/  
https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6  
https://towardsdatascience.com/coding-neural-network-forward-propagation-and-backpropagtion-ccf8cf369f76  







