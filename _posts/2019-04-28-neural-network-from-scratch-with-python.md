### Имплементация компьютерной нейронной сети с помощью Python  
{% include math.html %}
Всем привет. [В предыдущей статье](https://vkhvorostianyi.github.io/2019/04/14/logistic-regression-and-regularization.html) уже было упомянуто о том, что функция сигмоиды может использоваться как фукция активации компьютерной "нейронной сети", сегодня рассмотрим алгоритм "нейронки" более подробно. Компьютерная "нейронная сеть" (Computer Neural Network) является алгоритмом машинного обучения, который за своим принципом напоминает работу нейронной сети человека. В двух словах условный "нейрон" - это функция (активации), которая принимает и отдает сигналы.

Изображение "нейрона" (математическая модель парсептрона)
![img](/assets/neuron.png)

Нейроны-функции обьеденяются в слои, слоев может быть много и не очень, их расделяют на входной, скрытый(скрытые) и выходной, при этом нейронная сеть с одним скрытым (внутренним) слоем будет називатся однослойной.  
Виглядит это примерно так:  

![img](/assets/neuronnaya-set.gif)

Нужно добавить, что рассматриваемая нейронная сеть относится к типу "обучение с учителем" (supervised learning) и решает задачу классификации. 


#### Функция активации

Рассмотрим некоторые популярные функции активации.

*Сигмоида*

Как было сказано выше, функция сигмоиды часто используется как функция активации в нейронных сетях, наряду с гиперболическим тангенсои и ReLU(о которых поговорим ниже).

Ниже дана формула сигмоиды (в качестве примера степенной функции взята линейная регрессия):

$$z^{(i)} = w^T x^{(i)} + b $$
 
$$\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$

$$sigmoid(z^{(i)}) = \frac{1}{1-e^{-z^{(i)}}}$$


Где   
$$z^{(i)}$$ - степенная функция (линейная регрессия)  
$$\hat{y}^{(i)}$$ - гипотеза   
$$sigmoid(z^{(i)})$$ - сигмоида 

Графичек:  
![img](/assets/sigmoid.png)

Преимущества сигмоиды:
- Гладкость: Имеет производную в любой точке, что является важным фактором при использовании метода градиетного спуска
- Чуствительна к малейшим изменениям $$x$$ на участке от [-2;2], что позволяет четко провести границу между классами.
- Значения $$y$$ всегда находятся в диапазоне [0,1]

Недостатки:
- Стоимость вычислений, критично на устроиствах с ограниченымы ресурсами(IoT)
- Если в промежутке [-2,2] кривая сигмоиды изменяет значения "быстро" то по краях фукция остается практически неизменной, это говорит о том, что при высоких порогах, будет крайне сложно получить значения, которые близки к еденице, нейронная сеть будет обучаться долго и малоэфективно. Эта проблема также известна, как проблема изчезающего градиента, так как ближе к границам сигмоидальной функции, градиент стремится к нулю.
- 

Графики различных ф-ций активации:  

![img](/assets/actviation_func.png)


#### Прямое распространение (Forward Propagation)  

Прямое распостранение - это собстевенно, пропуск данных через мясорубку скрытых слоев нейронов.
